# TableFormer: Table Structure Understanding with Transformers

Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, Peter Staar
IBM Research
{ahn,nli,mly,taa}@zurich.ibm.com

## Abstract

Tables organize valuable content in a concise and compact representation. This content is extremely valuable for modern search engines, Knowledge Graph's, etc. since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multi-line rows, different variety of separation lines, missing entries, etc. As such, the correct identification of the table structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-decoder from PubTabNet) in two significant ways. First, we introduce a new object detector for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF's directly from the PDF source and avoid the training of the cumbersome OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to handle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art Tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.

## 1. Introduction

The occurrence of tables in documents is ubiquitous. They often summarize quantitative or factual data, which is cumbersome to describe in verbose text but nevertheless extremely valuable. Unfortunately, this compact representation is often not easy to parse by machines. There are many implicit conventions used to obtain a compact table representation. For example, tables often have complex column- and row-headers in order to reduce duplicated cell content. Lines of different shapes and sizes are leveraged to separate content or indicate a tree structure. Additionally, tables can also have empty/missing table-entries or multi-row textual table-entries. Fig. 1 shows a table which presents all these issues.

### Figure 1: Picture of a table with subtle, complex features

Picture of table showing (1) multi-column headers, (2) cell with multi-row text and (3) cells with no content. Image from PubTabNet evaluation set, filename: 'PMC2944238.004.02'.

a. Picture of a table:
- Shows Observer 1 and Observer 2 columns
- Contains entries marked as "benign" and "malignant"
- Total observer numbers

b. Red-annotation of bounding boxes, Blue-predictions by TableFormer

c. Structure predicted by TableFormer:
- Grid layout showing predicted table structure
- Numbered cells from 1-20

Recently, significant progress has been made with vision based approaches to extract tables in documents. For the sake of completeness, the issue of table extraction from documents is typically decomposed into two separate challenges, i.e.: (1) finding the location of the table(s) on a document-page and (2) finding the structure of a given table in the document.

The first problem is called table-location and has been previously addressed [30, 38, 19, 21, 23, 26, 8] with state-of-the-art object-detection networks (e.g. YOLO and later on Mask-RCNN [9]). For all practical purposes, it can be.

---

## 2. Previous work and State of the Art

Identifying the structure of a table has been an outstanding problem in the document analysis community, that motivates many organised public challenges [6, 4, 14]. The difficulty of the problem can be attributed to a number of factors. First, there is a large variety in the shapes and sizes of tables. Such large variety requires a flexible method. This is especially true for complex column- and row headers, which can be extremely intricate and demanding. A second factor of complexity is the lack of data with regard to table-structure annotation, especially due to the fact that tables are notoriously time-consuming to annotate by hand. However, this has definitely changed in recent years with the adherence of PubTabNet [37], FinTabNet [36], TableBank [17] etc.

Before the recent popularity of deep neural networks, the community relied heavily on heuristic and/or statistical methods to do table structure identification [3, 7, 11, 5, 13, 28]. Although such methods work well on constrained tables [12], a more data-driven approach can be applied due to the advent of computational neural networks (CNNs) and the availability of large datasets. To the best-of our knowledge, there are currently very different types of network architectures that are being pursued for state-of-the-art table-structure identification.

### Image-to-Text networks:

In this type of network, one predicts a sequence of tokens starting from an encoded image. Such sequences of tokens originate from marked-up languages such as HTML table tags [37, 17] or LaTeX symbols[10]. The choice of symbols is ultimately not very important, since one can transform into the other. There are however subtle variations in the Image-to-Text networks. The easiest network architectures are "image-encoder → text-decoder" (IETD), similar to network architectures that try to provide captions to images [32]. In these IETD networks, one expects as output the LaTeX/HTML string of the entire table, i.e. the symbols necessary for creating the table with the content of the table. Another approach is the "image-encoder → dual decoder" (IEDD) networks. In these type of networks, one has two consecutive decoders with different purposes. The first decoder is the tag-decoder, i.e. it produces the HTML/LaTeX tags which construct an empty table. The second content-decoder uses the encoding of the image in combination with the output encoding of the tag-cell-decoder) to generate the textual content of each table cell. The network architecture of IEDD is certainly more elaborate, but it has the advantage that one can pre-train the tag-decoder which is constrained to the table-tags.

In practice, both new-architectures (IETD and IEDD) require an implicit, custom trained object-character-recognition (OCR) to obtain the content of the table-cells. In the case of IETD, this OCR engine is implicit in the decoder similar to [24]. For the IEDD the OCR is solely embedded in the content-decoder. This reliance on a custom, implicit OCR decoder is of course problematic. OCR is a well known and extremely tough problem, that often needs custom training for each individual language. However, the limited availability for non-english content in the current datasets, makes it impractical to apply the IETD and IEDD methods on tables with other languages. Additionally, OCR can be completely controlled off the tables originate from programmatic PDF documents with known positions of each cell. The latter was the inspiration for the work of this paper.

### Graph Neural networks:

Graph Neural networks (GNNs) take a radically different approach to table-structure extraction. Note that one table cell can constitute one of multiple text-cells. To obtain the table-structure, one creates an initial graph, where each of the text-cells becomes a node in the graph similar to [33, 34, 2]. Each node is then associated with an embedding vector coming from the encoded image, its coordinates and the encoded text. Furthermore, nodes that represent adjacent text-cells are linked. Graph Convolutional Networks (GCN's) based methods take the image as an input, but also the position of the text-cells and their content [18]. The purpose of a GCN is to transform the input graph into a new graph, which replaces the old links with new ones. The new links then represent the table-structure. With this approach, one can avoid the need to build custom OCR decoders. However, the quality of the reconstructed structure is not comparable to the current state-of-the art [18].

### Hybrid Deep Learning-Based approach:

A popular current model for table-structure identification is the use of a deep learning-based approach similar to [27, 29]. In this approach, one first detects the position of the table-cells with object detection (e.g. YoloV3 or Mask-RCNN), then uses the bounding-boxes of its images) and finally uses different sub-sets to obtain the table-structure. Currently, this approach achieves state-of-the-art results, but is not an end-to-end deep learning method. As such, new rules need to be written if different types of tables are encountered.

## 3. Datasets

We rely on large-scale datasets such as PubTabNet [37], FinTabNet [36], and TableBank [17] datasets to train and evaluate our models. These datasets span over various appearance styles and content. We also introduce our own synthetically generated SynthTabNet dataset to fix an important limitation of the previous datasets.

The availability of the bounding boxes for all table cells is essential to train our models. In order to distinguish between empty and non-empty bounding boxes, we have introduced a binary class in the annotation. Unfortunately, the original datasets offer either offer the bounding boxes for a whole table (e.g. TableBank) or they narrow their scope only to non-empty cells. Therefore, it was imperative to introduce a data pre-processing procedure that generates the missing bounding boxes out of the annotation information. This procedure first passes the provided table structure and calculates the dimensions of the most fine-grained grid that covers the whole structure. Notice that each table cell will now occupy multiple grid squares due to row or column spans. In the case of PubTabNet we had to compute missing bounding boxes for 48% of the simple and 69% of the complex tables. Regarding FinTabNet, 68% of the simple and 98% of the complex tables require the generation of bounding boxes.

As it is illustrated in Fig. 2, the table distributions from all datasets are skewed towards simpler structures with fewer number of rows/columns. Additionally, there is very limited variance in the table styles, which in case of PubTabNet and FinTabNet means one styling format for the majority of the tables. Similar limitations appear also in the type of table content, which in some cases (e.g. FinTabNet) is restricted to a certain domain. Ultimately, the lack of diversity in the training dataset damages the ability of the models to generalize well on unseen data.

Motivated by those observations we aimed at generating a synthetic table dataset named SynthTabNet. This approach offers control over: 1) the size of the dataset, 2) the table structure, 3) the table style and 4) the type of content. The complexity of the table structure is described by the size of the table header and the table body, as well as the percentage of the table cells covered by row spans and column spans. A set of carefully designed styling templates provides the basis for building a large range of table appearances. Lastly, the table content is generated out of a curated collection of text corpora. By controlling the size and scope of the synthetic datasets we are able to train our models in a variety of different conditions. For example, we can first generate a highly diverse dataset to train our models, and then evaluate their performance on other synthetic datasets which are focused on a specific domain.

In this regard, we have prepared four synthetic datasets, each one containing 150k examples. The corpora to generate the table text consist of the most frequent terms appearing in PubTabNet and FinTabNet together with randomly generated text. The first two synthetic datasets have been fine-tuned to mimic the appearance of the original datasets but encompass more complicated table structures. The third dataset encompasses a very large variety of different conditions. For example, we can first generate a highly diverse dataset to train our models, and then evaluate their performance on other synthetic datasets which are focused on a specific domain.

### Table 1: Dataset Attributes

| Dataset | Tags | BBoxes | Size | Format |
|---------|------|--------|------|--------|
| PubTabNet | ✓ | ✓ | 509k | PNG |
| FinTabNet | ✓ | ✓ | 112k | PDF |
| TableBank | ✓ | ✗ | 145k | JPEG |
| Combined-Tabnet(*) | ✓ | ✓ | 400k | PNG |
| Combined-Tabnet(**) | ✓ | ✓ | 500k | PNG |
| SynthTabNet | ✓ | ✓ | 600k | PNG |

Both "Combined-Tabnet" and "Combined-Tabnet" are variations of the following: (*) The Combined-Tabnet dataset is the processed combination of PubTabNet and FinTabnet. (**) The combined dataset is the processed combination of PubTabNet, FinTabnet, and TableBank.

The distribution of the tables across different table dimensions in PubTabNet + FinTabNet datasets.

### 4. The TableFormer model

Given the image of a table, TableFormer is able to predict: 1) a sequence of tokens that represents the structure of a table, and 2) a bounding box coupled to a subset of those tokens. The conversion of an image into a sequence of tokens is a well-known task [25, 16]. While attention is often used as an implicit method to associate each token of the sequence with a position in the original image, an explicit association between the individual table-cells and the image bounding boxes is also required.

#### 4.1. Model architecture

We now describe in detail the proposed method, which is composed of three main components, see Fig. 4. Our CNN Backbone Network encodes the input as a feature vector of predefined length. The input feature vector of the encoded image is passed to the Structure Decoder to produce a sequence of HTML tags that represent the structure of the table. With each prediction of an HTML standard data cell ('<td>') the hidden state of that cell is passed to the Cell BBox Decoder. As for spanning cells, such as row or column span, the tag is broken down to '<', 'rowspan' or 'colspan' with the number of spanning cells (attribute), and '>'. The hidden state attached to the '<' is passed to the Cell BBox Decoder. A shared feed forward network (FFN) receives the hidden states from the Structure Decoder, to provide the final detection predictions of the bounding box coordinates and their classification.

#### CNN Backbone Network

A ResNet-18 CNN is the backbone that receives the table image and encodes it as a vector of predefined length. The network has been modified by removing the linear and pooling layer, as we are not performing classification, and adding an adaptive pooling layer of size 28*28. ResNet by default downsamples the image resolution by 32 after the encoded image is provided to both the Structure Decoder, and Cell BBox Decoder.

#### Structure Decoder

The transformer architecture of this component is based on the work proposed in [31]. After extensive experimentation, the Structure Decoder is modeled as a transformer decoder made from a stack of 4 decoder layers that comprise mainly of multi-head attention and feed forward layers. This configuration uses fewer heads in comparison to networks applied to other problems (e.g. "Scene Understanding", "Image Captioning"), something which we relate to the simplicity of table images.

The transformer module receives an encoded image from the CNN Backbone Network and refines it through a multi-head dot-product attention layer, followed by a Feed Forward Network. During training, the transformer decoder receives as input the output feature produced by the transformer encoder, and the tokenized input of the HTML ground-truth tags. Using a stack of multi-head attention layers, different aspects of the tag sequence could be inferred. This is achieved by each attention head on a layer operating in a different subspace, and then combining altogether their attention scores.

#### Cell BBox Decoder

Our architecture allows us to simultaneously predict HTML tags and bounding boxes for each table cell within the model end to end. This approach is inspired by DETR [1] which employs a Transformer Encoder-Decoder that looks for a specific number of object queries (potential object detections). As our model utilizes a transformer architecture, the hidden state of the '<d>' and '<' HTML structure tags becomes the object query.

The encoding generated by the CNN Backbone Network along with the features acquired for every data cell from the Transformer Decoder are then passed to the attention network. The attention network takes both inputs and learns to provide an attention weighted encoding. This weighted at-
